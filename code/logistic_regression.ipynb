{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2804969e-b4b0-41e4-8851-4f1535076355",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Useful starting lines\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "\n",
    "import sys\n",
    "\n",
    "sys.path.append('../')\n",
    "\n",
    "from implementations import *\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6d5c6d5-ff23-4548-b0cc-605ad0951aa5",
   "metadata": {},
   "source": [
    "# **Load and clean the training data**\n",
    "\n",
    "We load the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "122f5c21-3d35-4ffa-be0a-ceb554346465",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The data has 328135 samples and 321 features !\n"
     ]
    }
   ],
   "source": [
    "x_train, x_test, y_train, train_ids, test_ids = load_csv_data(\"../data\")\n",
    "\n",
    "print(f\"The data has {x_train.shape[0]} samples and {x_train.shape[1]} features !\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b72a59af-6d28-4d6d-9f2a-748284176c28",
   "metadata": {},
   "source": [
    "For each feature, we clean the data so the values make more sense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "80aeb7aa-aadb-4470-9f8c-3f24a58e5f24",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#TODO : clean the data by calling the associated function in the implementations.py\n",
    "x_train[:,6] = replace(x_train[:,6], [1100,1200], [1,0])\n",
    "x_train[:,13] = replace(x_train[:,13], [0,1], [1,2])\n",
    "x_train[:,24] = replace(x_train[:,24], [1,2,7,9], [0,1,np.nan,np.nan])\n",
    "x_train[:,25] = replace(x_train[:,25], [77,99], [np.nan,np.nan])\n",
    "x_train[:,26] = replace(x_train[:,26], [2,3,4,5,7,9], [0.75,0.5,0.25,0,np.nan,np.nan])\n",
    "\n",
    "array_1 = [27,28,29]\n",
    "\n",
    "for i in array_1 : \n",
    "    x_train[:,i] = replace(x_train[:,i], [88,77,99], [np.nan,np.nan,np.nan])\n",
    "\n",
    "x_train[:,31] = replace(x_train[:,31], [3,7,9], [0,np.nan,np.nan])\n",
    "\n",
    "array_2 = [30,32,34,35,36,38,39,40,41,42,43,44,45,46,47,48,53,54,55,56,57,61,64,65,66,67,68,69,70,71,72,73,74,87,95,96,100,103,104,107,108,116,117,118]\n",
    "           \n",
    "for i in array_2:\n",
    "    x_train[:,i] = replace(x_train[:,i], [7,9], [np.nan,np.nan])\n",
    "    \n",
    "\n",
    "x_train[:,33] = replace(x_train[:,33], [1,2,3,4,7,8,9], [6,18,42,60,np.nan,120,np.nan])\n",
    "x_train[:,37] = replace(x_train[:,37], [1,2,3,4,7,9], [6,18,42,60,np.nan,np.nan])\n",
    "x_train[:,49] = replace(x_train[:,49], [98,99], [np.nan,np.nan])\n",
    "\n",
    "array_3 = [51,52,58]\n",
    "\n",
    "for i in array_3 : \n",
    "    x_train[:,i] = replace(x_train[:,i], [9], [np.nan])\n",
    "    \n",
    "x_train[:,59] = replace(x_train[:,59], [88,99], [0,np.nan])\n",
    "x_train[:,60] = replace(x_train[:,60], [1,2,3,4,5,6,7,8,77,99] , [5,12.5,17.5,22.5,30,42.5,62.5,75,np.nan,np.nan])\n",
    "\n",
    "x_train[:,62] = replace(x_train[:,62], [7777,9999], [np.nan,np.nan])    \n",
    "x_train[:,62] = list(map(IntoPounds,(x_train[:, 62])))\n",
    "\n",
    "x_train[:,63] = replace(x_train[:,63], [7777,9999], [np.nan,np.nan])    \n",
    "x_train[:,63] = list(map(IntoInches,(x_train[:, 63])))\n",
    "\n",
    "x_train[:,75] = replace(x_train[:,75],[1,2,3,4,5,6,7,8,77,99] , [15,60,135,270,1080,2070,3600,np.nan,np.nan,np.nan])\n",
    "x_train[:,76] = replace(x_train[:,76],[3,7,9] ,[0,np.nan,np.nan])\n",
    "x_train[:,77] = replace(x_train[:,77],[777,888,999] ,[np.nan,0,np.nan])\n",
    "x_train[:,77] = list(map(WeekToMonth,(x_train[:, 77])))\n",
    "\n",
    "\n",
    "array_5 = [78,80,88,91,98,119]\n",
    "\n",
    "for i in array_5 :\n",
    "    x_train[:,i] = replace(x_train[:,i], [77,99], [np.nan,np.nan])\n",
    "    \n",
    "x_train[:,79] = replace(x_train[:,79],[77,88,99] ,[np.nan,0,np.nan])\n",
    "\n",
    "array_6 = [81,82,83,84,85,86]\n",
    "\n",
    "for i in array_6 :\n",
    "    x_train[:,i] = replace(x_train[:,i], [300,555,777,999], [0,0,np.nan,np.nan])\n",
    "    x_train[:,i] = list(map(DayToMonth,(x_train[:, i])))\n",
    "    \n",
    "array_7 = [89,90,92,93] \n",
    "\n",
    "for i in array_7 :\n",
    "    x_train[:,i] = replace(x_train[:,i], [777,999],  [0,0,np.nan,np.nan])\n",
    "\n",
    "x_train[:,89] = list(map(WeekToMonth,(x_train[:, 89])))\n",
    "x_train[:,90] = list(map(HourToMinutes,(x_train[:, 90])))\n",
    "x_train[:,92] = list(map(HourToMinutes,(x_train[:, 92])))\n",
    "\n",
    "array_8 = [94,110,111] \n",
    "\n",
    "for i in array_8 :\n",
    "    x_train[:,i] = replace(x_train[:,i], [777,888,999], [np.nan,0,np.nan])\n",
    "\n",
    "x_train[:,94] = replace(x_train[:,94], [777,888,999], [np.nan,0,np.nan])\n",
    "x_train[:,94] = list(map(WeekToMonth,(x_train[:, 94])))\n",
    "x_train[:,97] = replace(x_train[:,97], [2,3,7,9], [0.5,0,np.nan,np.nan])\n",
    "x_train[:,99] = replace(x_train[:,99], [2,3,4,5,7,8,9], [0.75,0.5,0.25,0,np.nan,np.nan,np.nan])\n",
    "x_train[:,101] = replace(x_train[:,101], [777777, 999999],  [np.nan,np.nan])\n",
    "\n",
    "#x_train[:,101] = list(map(DateType,(x_train[:, 101])))\n",
    "\n",
    "x_train[:,105] = replace(x_train[:,105], [777777, 999999],  [np.nan,np.nan])\n",
    "#x_train[:,105] = list(map(DateType,(x_train[:, 105])))\n",
    "\n",
    "x_train[:,110] = list(map(DayToYear,(x_train[:, 110])))\n",
    "x_train[:,111] = list(map(DayToYear,(x_train[:, 111])))\n",
    "\n",
    "x_train[:,113] = replace(x_train[:,113],[77,88,98,99] ,[np.nan,0,np.nan,np.nan])\n",
    "x_train[:,114] = replace(x_train[:,114],[77,88,99] ,[np.nan,0,np.nan])\n",
    "x_train[:,115] = replace(x_train[:,114],[1,2,3,4,7,8,9] ,[15,180,540,720,np.nan,0,np.nan])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fc51dcba-5484-405b-80aa-d02516f82806",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "x_train[:, 240] = replace(x_train[:, 240], [np.nan, 77, 99], [-1, -1, -1])\n",
    "x_train[:, 246] = replace(x_train[:, 246], [np.nan, 14], [-1, -1])\n",
    "x_train[:, 247] = replace(x_train[:, 247], [np.nan, 3], [-1, -1])\n",
    "x_train[:, 252] = replace(x_train[:, 252], [np.nan, 99999], [np.nan, np.nan])\n",
    "x_train[:, 261] = replace(x_train[:, 261], [np.nan, 7, 9], [-1, -1, -1])\n",
    "x_train[:, 262] = replace(x_train[:, 262], [np.nan, 900], [-1, -1])\n",
    "x_train[:, 298] = replace(x_train[:, 298], [np.nan, 9], [1, 1])\n",
    "\n",
    "rep_one = [241, 242, 243, 244, 255, 256, 257, 258, 259, 260, 263, 265, 278, 279, 284, \n",
    "          305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320]\n",
    "\n",
    "for i in rep_one : \n",
    "    x_train[:, i] = replace(x_train[:, i], [np.nan, 9], [-1, -1])\n",
    "    \n",
    "rep_two = [245, 249, 254, 289, 290, 291, 292]\n",
    "\n",
    "for i in rep_two : \n",
    "    x_train[:, i] = replace(x_train[:, i], [np.nan], [-1])\n",
    "    \n",
    "\n",
    "x_train[:, 264] = replace(x_train[:, 264], [np.nan, 99900], [-1, -1])\n",
    "x_train[:, 287] = replace(x_train[:, 287], [np.nan, 99900], [-1, -1])\n",
    "x_train[:, 288] = replace(x_train[:, 288], [np.nan, 99900], [-1, -1])\n",
    "\n",
    "x_train[:, 272] = replace(x_train[:, 272], [np.nan], [0])\n",
    "x_train[:, 273] = replace(x_train[:, 273], [np.nan], [0])\n",
    "\n",
    "x_train[:, 274] = replace(x_train[:, 274], [np.nan], [1])\n",
    "x_train[:, 275] = replace(x_train[:, 275], [np.nan], [1])\n",
    "x_train[:, 280] = replace(x_train[:, 280], [np.nan], [1])\n",
    "x_train[:, 281] = replace(x_train[:, 281], [np.nan], [1])\n",
    "\n",
    "x_train[:, 282] = replace(x_train[:, 282], [np.nan], [2])\n",
    "x_train[:, 283] = replace(x_train[:, 283], [np.nan], [2])\n",
    "\n",
    "x_train[:, 293] = replace(x_train[:, 293], [np.nan, 99000], [np.nan, np.nan])\n",
    "x_train[:, 294] = replace(x_train[:, 294], [np.nan, 99000], [np.nan, np.nan])\n",
    "x_train[:, 297] = replace(x_train[:, 297], [np.nan, 99000], [np.nan, np.nan])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0af58328-3e1e-4ab3-a14c-48dceb0e7f6d",
   "metadata": {},
   "source": [
    "We then clean the data by : \n",
    "- removing the nan values by the mean of the rest of the feature\n",
    "- removing the features where the variance is zero since they are constants for all samples\n",
    "- remove the 8 first features as the appear weird in the task of predicting a heart attack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "19d4ae66-b516-4139-a8f2-2512e785e3bc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The data has now 314 features !\n"
     ]
    }
   ],
   "source": [
    "x_train, zero_var_features = gen_clean(x_train)\n",
    "print(f\"The data has now {x_train.shape[1]} features !\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2183b026-23d6-4c69-971a-1d6f7690d029",
   "metadata": {},
   "source": [
    "# PCA algorithm implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "14a510c5-61e7-4087-b63a-8fa1a0dbbb82",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We can keep the 127 first most influent features given by pca_indices\n"
     ]
    }
   ],
   "source": [
    "pca_indices, idx = pca(x_train)\n",
    "print(f\"We can keep the {idx} first most influent features given by pca_indices\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c2484956-3861-4d1d-b7f5-a4eb1fde6290",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The data has now 128 features\n"
     ]
    }
   ],
   "source": [
    "x_train_pca = x_train[:, pca_indices]\n",
    "x_train_pca = x_train_pca[:, :(idx + 1)]\n",
    "print(f\"The data has now {x_train_pca.shape[1]} features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8528a1e6-524f-4810-9e74-fab4643346dd",
   "metadata": {},
   "source": [
    "# Logistic regression using stochastic gradient descent (SGD)\n",
    "\n",
    "We train our model using logistic regression using SGD with mean-square error.\n",
    "\n",
    "First, we separate our data in a training set(70%) and testing set(30%)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d70178b3-4a8b-4784-bf84-6b59ac3dacde",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tx_tr shape : (229694, 128) ; tx_te shape : (98441, 128)\n",
      "       y_tr : (229694,)     ;        y_te : (98441,)\n"
     ]
    }
   ],
   "source": [
    "tx_tr, tx_te, y_tr, y_te = cross(x_train_pca, y_train, 0.7)\n",
    "\n",
    "print(f\"tx_tr shape : {tx_tr.shape} ; tx_te shape : {tx_te.shape}\")\n",
    "print(f\"       y_tr : {y_tr.shape}     ;        y_te : {y_te.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0779560-bcd8-4c8e-a440-7a6c831df032",
   "metadata": {},
   "source": [
    "Now we build our models for linear regression using SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4e8245c8-0473-432d-804d-a33d2aa8b190",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The data has now 129 features !\n"
     ]
    }
   ],
   "source": [
    "y, tx = build_model_data(tx_tr, y_tr)\n",
    "y_test, tx_test = build_model_data(tx_te, y_te)\n",
    "print(f\"The data has now {tx.shape[1]} features !\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ad76574-befc-4279-b387-98b2b18822a8",
   "metadata": {},
   "source": [
    "## Training \n",
    "\n",
    "Here we train our model using SGD with MSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bd671ffe-d060-4087-8049-2504a03fce9f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution time 68.810316 seconds\n"
     ]
    }
   ],
   "source": [
    "initial_w = np.zeros(tx.shape[1])\n",
    "max_iters = 100\n",
    "gamma = 0.009\n",
    "\n",
    "start_time = datetime.datetime.now()\n",
    "\n",
    "w, loss = mean_squared_error_sgd(y, tx, initial_w, max_iters, gamma)\n",
    "\n",
    "\n",
    "end_time = datetime.datetime.now()\n",
    "print(f\"Execution time {(end_time - start_time).total_seconds()} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a50a196-c27e-42e0-b245-ca55a7a82ba9",
   "metadata": {},
   "source": [
    "### Computation of metrics\n",
    "\n",
    "We first compute some metrics on the training data (60% of the total data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c8caf34b-65d1-4bc8-a682-a226626d8b53",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 86.03402788057154%\n",
      "Precision: 27.350427350427353%\n",
      "Recall : 35.472855941010536%\n",
      "F1-score : 30.886566842615533%\n"
     ]
    }
   ],
   "source": [
    "pred_data = np.dot(tx, w)\n",
    "\n",
    "pred_data[pred_data > 0] = 1\n",
    "pred_data[pred_data < 0] = -1\n",
    "\n",
    "correctly_classified_data = np.sum(pred_data == y)\n",
    "\n",
    "tp = np.sum((pred_data == 1) & (y == 1))\n",
    "fp = np.sum((pred_data == 1) & (y == -1))\n",
    "\n",
    "tn = np.sum((pred_data == -1) & (y == -1))\n",
    "fn = np.sum((pred_data == -1) & (y == 1))\n",
    "\n",
    "accuracy_data = (tp + tn)/(tp + fp + tn + fn)\n",
    "\n",
    "print(f\"Accuracy: {accuracy_data*100}%\")\n",
    "print(f\"Precision: {tp/(tp + fp)*100}%\")\n",
    "print(f\"Recall : {tp/(tp + fn)*100}%\")\n",
    "print(f\"F1-score : {tp/(tp + 0.5*(fn + fp))*100}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4f5dea0-18a8-4df9-be75-271fdec36923",
   "metadata": {},
   "source": [
    "Now we compute some metrics for our test data (40% of the total data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "12e0ac7d-077c-4d10-9fb2-ea0fad434b4f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 85.99872004550949%\n",
      "Precision: 27.87825319805911%\n",
      "Recall : 36.04014598540146%\n",
      "F1-score : 31.43809381684326%\n"
     ]
    }
   ],
   "source": [
    "pred_test = np.dot(tx_test, w)\n",
    "\n",
    "pred_test[pred_test > 0] = 1\n",
    "pred_test[pred_test < 0] = -1\n",
    "\n",
    "correctly_classified_test = np.sum(pred_test == y_test)\n",
    "\n",
    "tp = np.sum((pred_test == 1) & (y_test == 1))\n",
    "fp = np.sum((pred_test == 1) & (y_test == -1))\n",
    "\n",
    "tn = np.sum((pred_test == -1) & (y_test == -1))\n",
    "fn = np.sum((pred_test == -1) & (y_test == 1))\n",
    "\n",
    "accuracy_test = (tp + tn)/(tp + fp + tn + fn)\n",
    "\n",
    "print(f\"Accuracy: {accuracy_test*100}%\")\n",
    "print(f\"Precision: {tp/(tp + fp)*100}%\")\n",
    "print(f\"Recall : {tp/(tp + fn)*100}%\")\n",
    "print(f\"F1-score : {tp/(tp + 0.5*(fn + fp))*100}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1beeb38f-76e5-47e0-a81b-e816daf673d4",
   "metadata": {},
   "source": [
    "### Feature analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0f8b7102-06ad-4b7d-ad55-90aaeb3e7a89",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mplt\u001b[49m\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m10\u001b[39m, \u001b[38;5;241m10\u001b[39m))\n\u001b[0;32m      2\u001b[0m plt\u001b[38;5;241m.\u001b[39mbar(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(w)), np\u001b[38;5;241m.\u001b[39mabs(w))\n\u001b[0;32m      3\u001b[0m plt\u001b[38;5;241m.\u001b[39myscale(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlog\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'plt' is not defined"
     ]
    }
   ],
   "source": [
    "plt.figure(figsize=(10, 10))\n",
    "plt.bar(range(len(w)), np.abs(w))\n",
    "plt.yscale('log')\n",
    "plt.xlabel('Feature Index')\n",
    "plt.ylabel('Weight Value')\n",
    "plt.title('Feature Weights for Analysis')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40d7fb00-7b20-4af7-a5da-d94da53ae980",
   "metadata": {},
   "source": [
    "# Prediction on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "63e483fd-5a01-4703-8f04-bdc8e51989cf",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution time 1.955108 seconds\n"
     ]
    }
   ],
   "source": [
    "start_time = datetime.datetime.now()\n",
    "\n",
    "data_te = np.ones(x_test.shape)\n",
    "\n",
    "for i in range(data_te.shape[1]):\n",
    "    d, _ = standardize_clean(x_test[:, i])\n",
    "    data_te[:, i] = d\n",
    "    \n",
    "data_var_te = data_te[:, zero_var_features]\n",
    "data_var_te = np.squeeze(data_var_te, axis = 1)\n",
    "data_cleaned_te = data_var_te[:, 9:]\n",
    "\n",
    "data_test = data_cleaned_te[:, pca_indices]\n",
    "data_test = data_test[:, :(idx + 1)]\n",
    "\n",
    "end_time = datetime.datetime.now()\n",
    "print(f\"Execution time {(end_time - start_time).total_seconds()} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3a0eba11-ae4a-4888-81f2-44cd14cc9690",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(109379, 134)\n"
     ]
    }
   ],
   "source": [
    "tx_te = np.c_[np.ones(data_test.shape[0]), data_test]\n",
    "print(tx_te.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a73f619c-256a-4078-b54c-68a4cfc2c41e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(109379,)\n"
     ]
    }
   ],
   "source": [
    "y_te = np.dot(tx_te, w)\n",
    "print(y_te.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fc0f256b-6ef1-4fd1-964b-2c8cbc0c7696",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pred_te = y_te\n",
    "\n",
    "pred_te[pred_te > 0] = 1\n",
    "pred_te[pred_te < 0] = -1\n",
    "\n",
    "indices_one = np.where(pred_te == 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b0548600-a0f1-4cde-ac48-8b8499dc9a99",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "create_csv_submission(test_ids, pred_te, \"../data/sgd_reg.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
