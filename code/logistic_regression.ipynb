{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2804969e-b4b0-41e4-8851-4f1535076355",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# Useful starting lines\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "\n",
    "import sys\n",
    "\n",
    "sys.path.append('../')\n",
    "\n",
    "from implementations import *\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6d5c6d5-ff23-4548-b0cc-605ad0951aa5",
   "metadata": {},
   "source": [
    "# **Load and clean the training data**\n",
    "\n",
    "We load the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "122f5c21-3d35-4ffa-be0a-ceb554346465",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The data has 328135 samples and 321 features !\n"
     ]
    }
   ],
   "source": [
    "x_train, x_test, y_train, train_ids, test_ids = load_csv_data(\"../data\")\n",
    "\n",
    "print(f\"The data has {x_train.shape[0]} samples and {x_train.shape[1]} features !\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0af58328-3e1e-4ab3-a14c-48dceb0e7f6d",
   "metadata": {},
   "source": [
    "We then clean the data by : \n",
    "- removing the nan values by the mean of the rest of the feature\n",
    "- removing the features where the variance is zero since they are constants for all samples\n",
    "- remove the 8 first features as the appear weird in the task of predicting a heart attack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "cont_data = [13, 15, 16, 17, 24 ,25 ,26 ,27 ,28 ,29 ,33 ,37,49 ,59 ,60 ,62 ,63 ,75 ,77 ,78 ,\n",
    "             79 ,80 ,81 ,82 ,83 ,84 ,85 ,86 ,89 ,90 ,92 ,93 ,94, 98 ,110 ,111 ,112 ,113 ,114 ,143\n",
    "            ,147 ,148 ,149 ,150 ,168 ,195 ,197 , 206 ,207 ,208 ,209 ,210 ,211 ,212 ,213 ,219, 220, 221\n",
    "            ,222, 226, 228, 229, 248 ,250 ,251 ,252 ,253 ,262 ,264 ,266,267,268,269,270,271,276,277,285,286,287,288,291,292, 293,294, 295, 296, 297, 299, 300, 301, 302, 303, 304]\n",
    "\n",
    "# Generate a list of all indices from 1 to 320\n",
    "all_indices = list(range(0, 321))\n",
    "\n",
    "# Use a list comprehension to filter out indices not in your array\n",
    "cat_data = [idx for idx in all_indices if idx not in cont_data]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The data has now 321 features !\n"
     ]
    }
   ],
   "source": [
    "xt_feat = x_train.copy()\n",
    "xt_feat = preprocessing(xt_feat)\n",
    "xt_feat = gen_binary(xt_feat, cat_data, cont_data)\n",
    "print(f\"The data has now {xt_feat.shape[1]} features !\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "19d4ae66-b516-4139-a8f2-2512e785e3bc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "xt_z = xt_feat.copy()\n",
    "for i in np.array([0, 1, 2, 3, 4, 5, 7, 8, 9, 10, 11, 12, 14, 19, 20, 21, 22, 23, 88, 91, 101, 105]):\n",
    "    xt_z[:, i] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature : 0\n",
      "Feature : 1\n",
      "Feature : 2\n",
      "Feature : 3\n",
      "Feature : 4\n",
      "Feature : 5\n",
      "Feature : 6\n",
      "Feature : 7\n",
      "Feature : 8\n",
      "Feature : 9\n",
      "Feature : 10\n",
      "Feature : 11\n",
      "Feature : 12\n",
      "Feature : 14\n",
      "Feature : 18\n",
      "Feature : 19\n",
      "Feature : 20\n",
      "Feature : 21\n",
      "Feature : 22\n",
      "Feature : 23\n",
      "Feature : 30\n",
      "Feature : 31\n",
      "Feature : 32\n",
      "Feature : 34\n",
      "Feature : 35\n",
      "Feature : 36\n",
      "Feature : 38\n",
      "Feature : 39\n",
      "Feature : 40\n",
      "Feature : 41\n",
      "Feature : 42\n",
      "Feature : 43\n",
      "Feature : 44\n",
      "Feature : 45\n",
      "Feature : 46\n",
      "Feature : 47\n",
      "Feature : 48\n",
      "Feature : 50\n",
      "Feature : 51\n",
      "Feature : 52\n",
      "Feature : 53\n",
      "Feature : 54\n",
      "Feature : 55\n",
      "Feature : 56\n",
      "Feature : 57\n",
      "Feature : 58\n",
      "Feature : 61\n",
      "Feature : 64\n",
      "Feature : 65\n",
      "Feature : 66\n",
      "Feature : 67\n",
      "Feature : 68\n",
      "Feature : 69\n",
      "Feature : 70\n",
      "Feature : 71\n",
      "Feature : 72\n",
      "Feature : 73\n",
      "Feature : 74\n",
      "Feature : 76\n",
      "Feature : 87\n",
      "Feature : 88\n",
      "Feature : 91\n",
      "Feature : 95\n",
      "Feature : 96\n",
      "Feature : 97\n",
      "Feature : 99\n",
      "Feature : 100\n",
      "Feature : 101\n",
      "Feature : 102\n",
      "Feature : 103\n",
      "Feature : 104\n",
      "Feature : 105\n",
      "Feature : 106\n",
      "Feature : 107\n",
      "Feature : 108\n",
      "Feature : 109\n",
      "Feature : 115\n",
      "Feature : 116\n",
      "Feature : 117\n",
      "Feature : 118\n",
      "Feature : 119\n",
      "Feature : 120\n",
      "Feature : 121\n",
      "Feature : 122\n",
      "Feature : 123\n",
      "Feature : 124\n",
      "Feature : 125\n",
      "Feature : 126\n",
      "Feature : 127\n",
      "Feature : 128\n",
      "Feature : 129\n",
      "Feature : 130\n",
      "Feature : 131\n",
      "Feature : 132\n",
      "Feature : 133\n",
      "Feature : 134\n",
      "Feature : 135\n",
      "Feature : 136\n",
      "Feature : 137\n",
      "Feature : 138\n",
      "Feature : 139\n",
      "Feature : 140\n",
      "Feature : 141\n",
      "Feature : 142\n",
      "Feature : 144\n",
      "Feature : 145\n",
      "Feature : 146\n",
      "Feature : 151\n",
      "Feature : 152\n",
      "Feature : 153\n",
      "Feature : 154\n",
      "Feature : 155\n",
      "Feature : 156\n",
      "Feature : 157\n",
      "Feature : 158\n",
      "Feature : 159\n",
      "Feature : 160\n",
      "Feature : 161\n",
      "Feature : 162\n",
      "Feature : 163\n",
      "Feature : 164\n",
      "Feature : 165\n",
      "Feature : 166\n",
      "Feature : 167\n",
      "Feature : 169\n",
      "Feature : 170\n",
      "Feature : 171\n",
      "Feature : 172\n",
      "Feature : 173\n",
      "Feature : 174\n",
      "Feature : 175\n",
      "Feature : 176\n",
      "Feature : 177\n",
      "Feature : 178\n",
      "Feature : 179\n",
      "Feature : 180\n",
      "Feature : 181\n",
      "Feature : 182\n",
      "Feature : 183\n",
      "Feature : 184\n",
      "Feature : 185\n",
      "Feature : 186\n",
      "Feature : 187\n",
      "Feature : 188\n",
      "Feature : 189\n",
      "Feature : 190\n",
      "Feature : 191\n",
      "Feature : 192\n",
      "Feature : 193\n",
      "Feature : 194\n",
      "Feature : 196\n",
      "Feature : 198\n",
      "Feature : 199\n",
      "Feature : 200\n",
      "Feature : 201\n",
      "Feature : 202\n",
      "Feature : 203\n",
      "Feature : 204\n",
      "Feature : 205\n",
      "Feature : 214\n",
      "Feature : 215\n",
      "Feature : 216\n",
      "Feature : 217\n",
      "Feature : 218\n",
      "Feature : 223\n",
      "Feature : 224\n",
      "Feature : 225\n",
      "Feature : 227\n",
      "Feature : 230\n",
      "Feature : 231\n",
      "Feature : 232\n",
      "Feature : 233\n",
      "Feature : 234\n",
      "Feature : 235\n",
      "Feature : 236\n",
      "Feature : 237\n",
      "Feature : 238\n",
      "Feature : 239\n",
      "Feature : 240\n",
      "Feature : 241\n",
      "Feature : 242\n",
      "Feature : 243\n",
      "Feature : 244\n",
      "Feature : 245\n",
      "Feature : 246\n",
      "Feature : 247\n",
      "Feature : 249\n",
      "Feature : 254\n",
      "Feature : 255\n",
      "Feature : 256\n",
      "Feature : 257\n",
      "Feature : 258\n",
      "Feature : 259\n",
      "Feature : 260\n",
      "Feature : 261\n",
      "Feature : 263\n",
      "Feature : 265\n",
      "Feature : 272\n",
      "Feature : 273\n",
      "Feature : 274\n",
      "Feature : 275\n",
      "Feature : 278\n",
      "Feature : 279\n",
      "Feature : 280\n",
      "Feature : 281\n",
      "Feature : 282\n",
      "Feature : 283\n",
      "Feature : 284\n",
      "Feature : 289\n",
      "Feature : 290\n",
      "Feature : 298\n",
      "Feature : 305\n",
      "Feature : 306\n",
      "Feature : 307\n",
      "Feature : 308\n",
      "Feature : 309\n",
      "Feature : 310\n",
      "Feature : 311\n",
      "Feature : 312\n",
      "Feature : 313\n",
      "Feature : 314\n",
      "Feature : 315\n",
      "Feature : 316\n",
      "Feature : 317\n",
      "Feature : 318\n",
      "Feature : 319\n",
      "Feature : 320\n",
      "The data has now 1041 features !\n"
     ]
    }
   ],
   "source": [
    "xt_feat_sep = cat_sep(xt_z, cat_data)\n",
    "print(f\"The data has now {xt_feat_sep.shape[1]} features !\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The data has now 814 features !\n"
     ]
    }
   ],
   "source": [
    "separated_categories = np.delete(xt_feat_sep, cat_data, axis=1)\n",
    "x_train_pca = separated_categories\n",
    "print(f\"The data has now {separated_categories.shape[1]} features !\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "id": "2183b026-23d6-4c69-971a-1d6f7690d029",
   "metadata": {},
   "source": [
    "# PCA algorithm implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "14a510c5-61e7-4087-b63a-8fa1a0dbbb82",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We can keep the 138 first most influent features given by pca_indices\n"
     ]
    }
   ],
   "source": [
    "pca_indices, idx = pca(separated_categories)\n",
    "print(f\"We can keep the {idx} first most influent features given by pca_indices\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c2484956-3861-4d1d-b7f5-a4eb1fde6290",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The data has now 138 features\n"
     ]
    }
   ],
   "source": [
    "x_train_pca = separated_categories[:, pca_indices]\n",
    "x_train_pca = separated_categories[:, :idx]\n",
    "\n",
    "print(f\"The data has now {x_train_pca.shape[1]} features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8528a1e6-524f-4810-9e74-fab4643346dd",
   "metadata": {},
   "source": [
    "# Logistic regression using stochastic gradient descent (SGD)\n",
    "\n",
    "We train our model using logistic regression using SGD with mean-square error.\n",
    "\n",
    "First, we separate our data in a training set(70%) and testing set(30%)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d70178b3-4a8b-4784-bf84-6b59ac3dacde",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tx_tr shape : (157504, 138) ; tx_te shape : (170631, 138)\n",
      "       y_tr : (157504,)     ;        y_te : (170631,)\n"
     ]
    }
   ],
   "source": [
    "tx_tr, tx_te, y_tr, y_te = cross(x_train_pca, y_train, 0.48)\n",
    "\n",
    "print(f\"tx_tr shape : {tx_tr.shape} ; tx_te shape : {tx_te.shape}\")\n",
    "print(f\"       y_tr : {y_tr.shape}     ;        y_te : {y_te.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0779560-bcd8-4c8e-a440-7a6c831df032",
   "metadata": {},
   "source": [
    "Now we build our models for linear regression using SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4e8245c8-0473-432d-804d-a33d2aa8b190",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The data has now 139 features !\n"
     ]
    }
   ],
   "source": [
    "y, tx = build_model_data(tx_tr, y_tr)\n",
    "y_test, tx_test = build_model_data(tx_te, y_te)\n",
    "print(f\"The data has now {tx.shape[1]} features !\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ad76574-befc-4279-b387-98b2b18822a8",
   "metadata": {},
   "source": [
    "## Training \n",
    "\n",
    "Here we train our model using SGD with MSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "235fd6aa-b9db-4b84-b598-1e9beca270bb",
   "metadata": {
    "tags": [],
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "initial_w = np.zeros(tx.shape[1])\n",
    "degree = range(1,4)\n",
    "gammas = np.logspace(-5, 0, 15)\n",
    "max_iters = 2000\n",
    "\n",
    "start_time = datetime.datetime.now()\n",
    "best_degree, best_gamma, best_loss = logistic_regression_demo(tx, y, gammas, degree, max_iters)\n",
    "end_time = datetime.datetime.now()\n",
    "\n",
    "print(\n",
    "    \"The best rmse of %.3f is obtained for a degree of %.f and a lambda of %.5f.\"\n",
    "    % (best_loss, best_degree, best_gamma)\n",
    ")\n",
    "\n",
    "print(f\"Execution time {(end_time - start_time).total_seconds()} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "95a07ea0-31f4-453a-b1c4-f8073d798d61",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current iteration=0, loss=0.6931471805599453\n",
      "Current iteration=100, loss=0.599260986701567\n"
     ]
    }
   ],
   "source": [
    "# best_gamma = 6e-06\n",
    "# max_iters = 1201\n",
    "best_gamma = 3e-05\n",
    "initial_w = np.zeros(tx.shape[1])\n",
    "max_iters = 801\n",
    "\n",
    "w, loss = logistic_regression(y, tx, initial_w, max_iters, best_gamma)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a50a196-c27e-42e0-b245-ca55a7a82ba9",
   "metadata": {},
   "source": [
    "### Computation of metrics\n",
    "\n",
    "We first compute some metrics on the training data (60% of the total data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "outputs": [],
   "source": [
    "best_thresh = best_threshold(y, tx, w)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c8caf34b-65d1-4bc8-a682-a226626d8b53",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 63.47457842340511%\n",
      "Precision: 14.545454545454545%\n",
      "Recall : 64.93845039826212%\n",
      "F1-score : 23.7673093487047%\n"
     ]
    }
   ],
   "source": [
    "pred_data = np.dot(tx, w)\n",
    "\n",
    "pred_data[pred_data > best_thresh] = 1\n",
    "pred_data[pred_data <= best_thresh] = -1\n",
    "\n",
    "correctly_classified_data = np.sum(pred_data == y)\n",
    "\n",
    "tp = np.sum((pred_data == 1) & (y == 1))\n",
    "fp = np.sum((pred_data == 1) & (y == -1))\n",
    "\n",
    "tn = np.sum((pred_data == -1) & (y == -1))\n",
    "fn = np.sum((pred_data == -1) & (y == 1))\n",
    "\n",
    "accuracy_data = (tp + tn)/(tp + fp + tn + fn)\n",
    "\n",
    "print(f\"Accuracy: {accuracy_data*100}%\")\n",
    "print(f\"Precision: {tp/(tp + fp)*100}%\")\n",
    "print(f\"Recall : {tp/(tp + fn)*100}%\")\n",
    "print(f\"F1-score : {tp/(tp + 0.5*(fn + fp))*100}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4f5dea0-18a8-4df9-be75-271fdec36923",
   "metadata": {},
   "source": [
    "Now we compute some metrics for our test data (40% of the total data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "12e0ac7d-077c-4d10-9fb2-ea0fad434b4f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 63.58281906570319%\n",
      "Precision: 14.741645900261208%\n",
      "Recall : 64.75436861193538%\n",
      "F1-score : 24.015945413859306%\n"
     ]
    }
   ],
   "source": [
    "pred_test = np.dot(tx_test, w)\n",
    "\n",
    "pred_test[pred_test > best_thresh] = 1\n",
    "pred_test[pred_test <= best_thresh] = -1\n",
    "\n",
    "correctly_classified_test = np.sum(pred_test == y_test)\n",
    "\n",
    "tp = np.sum((pred_test == 1) & (y_test == 1))\n",
    "fp = np.sum((pred_test == 1) & (y_test == -1))\n",
    "\n",
    "tn = np.sum((pred_test == -1) & (y_test == -1))\n",
    "fn = np.sum((pred_test == -1) & (y_test == 1))\n",
    "\n",
    "accuracy_test = (tp + tn)/(tp + fp + tn + fn)\n",
    "\n",
    "print(f\"Accuracy: {accuracy_test*100}%\")\n",
    "print(f\"Precision: {tp/(tp + fp)*100}%\")\n",
    "print(f\"Recall : {tp/(tp + fn)*100}%\")\n",
    "print(f\"F1-score : {tp/(tp + 0.5*(fn + fp))*100}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40d7fb00-7b20-4af7-a5da-d94da53ae980",
   "metadata": {},
   "source": [
    "# Prediction on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63e483fd-5a01-4703-8f04-bdc8e51989cf",
   "metadata": {
    "tags": [],
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "start_time = datetime.datetime.now()\n",
    "\n",
    "tx_test = x_test\n",
    "tx_test = preprocessing(x_test)\n",
    "tx_test = gen_clean(tx_test, [], np.arange(321))\n",
    "\n",
    "tx_test = tx_test[:, pca_indices]\n",
    "tx_test = tx_test[:, :idx]\n",
    "tx_test = np.c_[np.ones(tx_test.shape[0]), tx_test]\n",
    "\n",
    "end_time = datetime.datetime.now()\n",
    "print(f\"Execution time {(end_time - start_time).total_seconds()} seconds\")\n",
    "print(f\"The data has {tx_test.shape[0]} samples and {tx_test.shape[1]} features !\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a0eba11-ae4a-4888-81f2-44cd14cc9690",
   "metadata": {
    "tags": [],
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "pred_te = np.dot(tx_test, w)\n",
    "\n",
    "pred_te[pred_te > best_thresh] = 1\n",
    "pred_te[pred_te < best_thresh] = -1\n",
    "\n",
    "indices_one = np.where(pred_te == 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "b0548600-a0f1-4cde-ac48-8b8499dc9a99",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "create_csv_submission(test_ids, pred_te, \"../data/log_reg.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}